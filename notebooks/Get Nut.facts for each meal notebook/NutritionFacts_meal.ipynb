{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "\n",
    "API_KEY = 'your api'\n",
    "SEARCH_URL = 'https://api.nal.usda.gov/fdc/v1/foods/search'\n",
    "\n",
    "desired_nutrients = {\n",
    "    \"Energy\": \"calories\",\n",
    "    \"Total lipid (fat)\": \"Total fats\",\n",
    "    \"Carbohydrate, by difference\": \"Carbohydrate\",\n",
    "    \"Fiber, total dietary\": \"Fiber\",\n",
    "    \"Protein\": \"Protein\",\n",
    "    \"Cholesterol\": \"Cholesterol\",\n",
    "    \"Calcium, Ca\": \"Calcium\",\n",
    "    \"Iron, Fe\": \"Iron\",\n",
    "    \"Magnesium, Mg\": \"Magnesium\",\n",
    "    \"Potassium, K\": \"Potassium\",\n",
    "    \"Sodium, Na\": \"Sodium\",\n",
    "    \"Vitamin C, total ascorbic acid\": \"Vitamin C\"\n",
    "}\n",
    "\n",
    "nutrition_cache = {}\n",
    "\n",
    "def fetch_nutrition_data(query):\n",
    "    if query in nutrition_cache:\n",
    "        return nutrition_cache[query]\n",
    "    params = {\n",
    "        'api_key': API_KEY,\n",
    "        'query': query,\n",
    "        'pageSize': 1\n",
    "    }\n",
    "    max_retries = 3\n",
    "    retry_delay = 1\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(SEARCH_URL, params=params, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data.get('foods'):\n",
    "                    food_item = data['foods'][0]\n",
    "                    nutrients = {}\n",
    "                    for nutrient in food_item.get('foodNutrients', []):\n",
    "                        name = nutrient.get('nutrientName')\n",
    "                        value = nutrient.get('value', 0)\n",
    "                        if name and value is not None:\n",
    "                            nutrients[name] = value\n",
    "                    nutrition_cache[query] = nutrients\n",
    "                    return nutrients\n",
    "                else:\n",
    "                    print(f\"No results found for '{query}'.\")\n",
    "                    break\n",
    "            elif response.status_code == 429:\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error fetching data for '{query}': HTTP {response.status_code}\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request exception for '{query}': {e}\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2\n",
    "            continue\n",
    "    empty_result = {}\n",
    "    nutrition_cache[query] = empty_result\n",
    "    return empty_result\n",
    "\n",
    "def extract_unique_ingredients(df):\n",
    "    unique_ingredients = set()\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            if isinstance(row['NER'], str):\n",
    "                ingredients = json.loads(row['NER'])\n",
    "                for ingredient in ingredients:\n",
    "                    unique_ingredients.add(ingredient.strip())\n",
    "        except (json.JSONDecodeError, TypeError) as e:\n",
    "            print(f\"Error parsing row {idx}: {e}\")\n",
    "    return unique_ingredients\n",
    "\n",
    "def prefetch_ingredients(ingredients):\n",
    "    def fetch_and_store(ingredient):\n",
    "        if ingredient and ingredient not in nutrition_cache:\n",
    "            result = fetch_nutrition_data(ingredient)\n",
    "            print(f\"Fetched data for '{ingredient}'\")\n",
    "            time.sleep(0.5)\n",
    "            return ingredient, result\n",
    "        return None, None\n",
    "    print(f\"Prefetching nutrition data for {len(ingredients)} unique ingredients...\")\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(fetch_and_store, ingredient) for ingredient in ingredients if ingredient]\n",
    "        for future in futures:\n",
    "            ingredient, result = future.result()\n",
    "            if ingredient:\n",
    "                nutrition_cache[ingredient] = result\n",
    "    print(f\"Prefetching complete. Cached {len(nutrition_cache)} ingredients.\")\n",
    "\n",
    "def aggregate_nutrition(ner_entry):\n",
    "    if isinstance(ner_entry, str):\n",
    "        try:\n",
    "            ingredients = json.loads(ner_entry)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Error parsing NER entry:\", e)\n",
    "            ingredients = []\n",
    "    else:\n",
    "        ingredients = ner_entry\n",
    "    aggregated = {std_key: 0 for std_key in desired_nutrients.values()}\n",
    "    for ingredient in ingredients:\n",
    "        ingredient = ingredient.strip()\n",
    "        if not ingredient:\n",
    "            continue\n",
    "        nutrient_data = nutrition_cache.get(ingredient, {})\n",
    "        for usda_name, std_key in desired_nutrients.items():\n",
    "            aggregated[std_key] += nutrient_data.get(usda_name, 0)\n",
    "    return aggregated\n",
    "\n",
    "def process_in_batches(df, batch_size=500):\n",
    "    total_rows = len(df)\n",
    "    result_df = pd.DataFrame()\n",
    "    for start_idx in range(0, total_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_rows)\n",
    "        print(f\"Processing batch {start_idx+1} to {end_idx} of {total_rows}...\")\n",
    "        batch = df.iloc[start_idx:end_idx].copy()\n",
    "        aggregated_nutrition_list = []\n",
    "        for idx, row in batch.iterrows():\n",
    "            nutrition = aggregate_nutrition(row['NER'])\n",
    "            aggregated_nutrition_list.append(nutrition)\n",
    "            if (idx - start_idx + 1) % 100 == 0:\n",
    "                print(f\"  Processed {idx - start_idx + 1}/{end_idx - start_idx} rows in current batch\")\n",
    "        batch['nutrition'] = aggregated_nutrition_list\n",
    "        nutrient_order = [\n",
    "            \"calories\", \"Total fats\", \"Carbohydrate\", \"Fiber\", \n",
    "            \"Protein\", \"Cholesterol\", \"Calcium\", \"Iron\", \n",
    "            \"Magnesium\", \"Potassium\", \"Sodium\", \"Vitamin C\"\n",
    "        ]\n",
    "        nutrition_df = batch['nutrition'].apply(pd.Series)[nutrient_order]\n",
    "        batch = pd.concat([batch, nutrition_df], axis=1)\n",
    "        result_df = pd.concat([result_df, batch], ignore_index=True)\n",
    "        result_df.to_csv(f'food_dataset_with_nutrition_partial_{end_idx}.csv', index=False)\n",
    "        print(f\"Saved intermediate results up to row {end_idx}\")\n",
    "    return result_df\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv('filtered_recipes_data.csv')\n",
    "    print(\"Extracting unique ingredients...\")\n",
    "    unique_ingredients = extract_unique_ingredients(df)\n",
    "    print(f\"Found {len(unique_ingredients)} unique ingredients\")\n",
    "    prefetch_ingredients(unique_ingredients)\n",
    "    result_df = process_in_batches(df)\n",
    "    result_df.to_csv('food_dataset_with_nutriition.csv', index=False)\n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"Completed in {execution_time:.2f} seconds\")\n",
    "    print(f\"Average time per row: {execution_time/len(df):.2f} seconds\")\n",
    "    print(\"Updated CSV with the selected nutrition facts has been saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
